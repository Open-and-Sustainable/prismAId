[project]
name = "Use of LLM for systematic review"
author = "John Doe"
version = "1.0"

[project.configuration]
input_directory = "/path/to/txt/files" # where the .txt files are
results_file_name = "/path/to/save/results" # where results will be saved, the path must exists, file extension will be added
output_format = "json"  # Could be "csv" [default] or "json"
log_level = "low" # Could be "low" [default], "medium" showing entries on stdout, or "high" saving entries on file, see user manual for details
duplication = "no" # Could be "yes" or "no" [default]. It duplicates the manuscripts to review, hence running model queries twice. It is a feature useful for debugging purposes.
cot_justification = "no" # Could be "yes" or "no" [default]. It is the model justification in terms of chain of thought for the answers provided.
#batch_execution = "no"  # Could be "yes" or "no" [default]. Not yet implemented.

[project.llm]
provider = "OpenAI" # Could be 'OpenAI', 'GoogleAI', 'Cohere', or 'Anthropic'.
api_key = "" # If left empty, it will search for an API key in env variables. Adding a key here is useful for tracking costs per prokect through project keys
model = "" # see below: 
# OpenAI: 'gpt-3.5-turbo', 'gpt-4-turbo', 'gpt-4o', 'gpt-4o-mini', or '' [default]. Leave empty string (or remove key) if you want cost optimizatoin: it will use GPT-4o Mini as the model currently minimizing costs and with maximum limits on input tokens.
# GoogleAI: 'gemini-1.5-flash', 'gemini-1.5-pro', or 'gemini-1.0-pro', or '' [default]. Leave empty string (or remove key) if you want cost optimization: it will use Gemini 1.5 Flash as the model currently minimizing costs and with maximum limits on input tokens.
# Cohere: 'command-r-plus', 'command-r', 'command-light', 'command', or '' [default]. Leave empty string (or remove key) if you want cost optimization: it will use . 
temperature = 0.2 # Between 0 and 1 on OpenAI or 2 on GoogleAI. Low model temperature to decrease randomness and ensure replicability
tpm_limit = 0 # The maximum number of Tokens Per Minute before delaying prompts. If 0 [default], it will not be considered or delay prompts. Otherwise, this should be set to the lowest number of tokens per minute allowed by the model (or models) and user tier.
rpm_limit = 0 # The maximin number of Requests Per Minute before delaying prompts. If 0 [default], it will not be considered or delay prompts. Otherwise, this should be set to the lowest number of tokens per minute allowed by the model (or models) and user tier.

[prompt]
persona = "You are an experienced scientist working on a systematic review of the literature." # Some text telling the model what role should be played. Personas help in setting the expectation on the model role
task = "You are asked to map the concepts discussed in a scientific paper attached here." # This is the task that needs to be solved
expected_result = "You should output a JSON object with the following keys and possible values: " # This introduces the structure of the output in JSON as specified below in the [review] section
failsafe = "If the concepts neither are clearly discussed in the document nor they can be deduced from the text, respond with an empty '' value." # This is the fail-safe option to ask the model to not force answers in categories provided. PArticularly useful if values to keys below are nto complete.
definitions = "'Interest rate' is the percentage charged by a lender for borrowing money or earned by an investor on a deposit over a specific period, typically expressed annually." # This is a chance to define the concepts we are asking to the model, to avoid misconceptions.
example = "" # This is a chance to provide an example of the concepts we are asking to the model, to avoid misconceptions.

[review] # Review items -- defining the knowledge map that needs to be filled in
[review.1]
key = "interest rate"
values = [""]
[review.2]
key = "regression models"
values = ["yes", "no"]
[review.3]
key = "geographical scale"
values = ["world", "continent", "river basin"]
